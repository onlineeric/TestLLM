def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):
	# Tokenize
	input_ids = tokenizer.encode(
					text,
					return_tensors="pt",
					truncation=True,
					max_length=max_input_tokens
	)

	# Generate
	device = model.device
	generated_tokens_with_prompt = model.generate(
		input_ids=input_ids.to(device),
		max_length=max_output_tokens
	)

	# Decode
	generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)

	# Strip the prompt
	generated_text_answer = generated_text_with_prompt[0][len(text):]

	return generated_text_answer